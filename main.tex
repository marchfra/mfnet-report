\documentclass{unitothesis}

% \includeonly{}

% Packages for TikZ NNs
\usepackage{listofitems} % for \readlist to create arrays
\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

% Colors for TikZ NNs
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}

% Styles for TikZ NNs
\tikzset{
    >=latex, % for default LaTeX arrow head
    node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
    node bias/.style={node,black!90,draw=black,fill=black!25},
    node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
    node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
    node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
    connect/.style={thick,mydarkblue}, %,line cap=round
    connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
    node 0/.style={node bias}, % node styles, numbered for easy mapping with \nstyle
    node 1/.style={node in},
    node 2/.style={node hidden},
    node 3/.style={node out}
}
\def\nstyle{int(\curr<\Nnodlen?min(2,\curr):3)} % map layer number onto 1, 2, or 3

% Reset section counter after new part
\usepackage{chngcntr}
\counterwithin*{section}{part}

% Useful commands for this project
\newcommand{\J}{\mathcal{J}}
\newcommand{\mono}[1]{\texttt{#1}}
\newcommand{\mfnet}{\mono{mfnet}\xspace}
\newcommand{\pytorch}{\mono{PyTorch}\xspace}
\newcommand{\shape}[2]{$#1\times #2$}
\newcommand{\wrt}{with respect to\xspace}
\newcommand{\nin}{n_\text{in}}
\newcommand{\nout}{n_\text{out}}

% Acronyms
\acrodef{FFCNN}{Feedforward Fully Connected Neural Network}
\acrodef{MSE}{Mean Squared Error}
\acrodef{CE}{Cross Entropy}
\acrodef{NN}{Neural Network}
\acrodef{GD}{Gradient Descent}
\acrodef{SGD}{Stochastic Gradient Descent}

\DeclareSIUnit\pixel{px}

\author{Francesco Marchisotti}
\title{\centering\mfnet \sc-- A simple\\[0.5em] Machine Learning Library}
\aayear{2024/2025}

\begin{supervisors}
   \supervisor{}{}{\sc Matteo Osella}
\end{supervisors}

\begin{document}

\maketitlepage
\thispagestyle{empty}
\subsection*{\centering Abstract}
Modern machine learning libraries such as \pytorch and \mono{TensorFlow} are widely used in both academia and industry. They are highly optimized, feature-rich, and provide extensive support for various machine learning tasks. In addition to their extensive features, these libraries are designed to efficiently leverage modern GPUs to significantly speed up training and inference for large-scale machine learning models.

To gain a deeper understanding of the workings of such libraries, \mfnet was developed as a simplified neural network library from scratch. The main objective was to implement a functional framework, focusing on the core concept of backpropagation.

The effectiveness of the library was validated by analyzing learning curves on two benchmark tasks: regression and classification. The results demonstrate that \mfnet successfully learns and generalizes, confirming the correctness of its implementation.

\newpage

{\hypersetup{linkcolor=black}\tableofcontents}

\include{Files/signal-flow}

\part{Implementation of \mfnet}

This first part describes the implementation details of the \mfnet library.

All the code for this project is available on \href{https://www.github.com/marchfra/mfnet}{GitHub}.

\include{Files/Part1/tensor}
\include{Files/Part1/backprop}
\include{Files/Part1/layer}
\include{Files/Part1/loss}
\include{Files/Part1/nn-optim-data}
\include{Files/Part1/train-utils}
\include{Files/Part1/train}

\part{Comparison with \pytorch}

In order to check that all the implementation of \mfnet is correct, two simple tasks have been performed and compared with \pytorch: a regression task on the California Housing dataset and a classification task on the MNIST dataset.

The two tasks have been implemented as closely as possible in both libraries, using the same architecture, loss function and optimizer. Training hyperparameters such as learning rate and batch size were different between the two frameworks, as using the same values for both resulted in a more unstable training for one of the two libraries.

Note that the goal of the project was not to outperform \pytorch, but rather to compare the training process of \mfnet with that of a production machine learning framework and check that the overall behavior is consistent with the expectations of a functioning library.

The code for the comparison can be found on \href{https://github.com/marchfra/interface}{GitHub}.

\include{Files/Part2/regression}
\include{Files/Part2/classification}
\include{Files/Part2/exec-times}

\appendix

\newpage
\include{Files/Appendix/example}

\end{document}

% Part 2: comparison with pytorch
% \begin{enumerate}
%     \item Regression task
%     \item Classification task
% \end{enumerate}
