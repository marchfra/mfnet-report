\section{Layers} \label{sec:layer}

In \mfnet, a layer is a fundamental building block of the \acl{NN}. Each layer consists of a set of neurons, and it performs a specific transformation on the input data. The two main types of layers implemented in \mfnet are Linear layers and Activation layers.

% \begin{itemize}
%     \item \textbf{Linear Layer:} This layer applies a linear transformation to the input data, represented by the equation $y = Ax + b$, where $A$ is the weight matrix, $x$ is the input vector, $b$ is the bias vector, and $y$ is the output vector.
%     \item \textbf{Activation Layer:} This layer applies a non-linear activation function to the output of the linear layer. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.
% \end{itemize}

Layers are stacked together to form a complete \acl{NN}, with the output of one layer serving as the input to the next layer.  Each layer is responsible for maintaining its own parameters and computing gradients during the backpropagation process.

% Each of the layers in \mfnet expects the input to be in the form of a matrix with shape $(n_{\text{in}} + 1, m)$, where $n_{\text{in}}$ is the number of input features and $m$ is the number of input samples. The first row of this matrix is reserved for a bias term, which is always set to 1.

\subsection{Linear Layer}
\subsubsection{Forward pass}
The Linear layer in \mfnet applies a linear transformation to the input data, performing a change in dimensionality from $\nin$ to $\nout$, where $\nin$ is the number of input features and $\nout$ is the number of output features. Mathematically, this can be represented as:
\begin{equation}
    y = b + Wx
\end{equation}

where:
\begin{itemize}
    \item $x$ is the \shape{\nin}{1} input vector,
    \item $W$ is the \shape{\nout}{\nin} weight matrix,
    \item $b$ is the \shape{\nout}{1} bias vector, and
    \item $y$ is the \shape{\nout}{1} output vector.
\end{itemize}

\paragraph{Implementation Details} The actual implementation is a bit different: the first key difference is that the bias term is absorbed inside the weights matrix, and the input vector is augmented with an additional constant value of 1. The layer expects this ``bias feature'' to be already present in the input data, and propagates it to the next layer by adding a row of $\mqty(1 & 0 \cdots 0)$ to the weights matrix. This allows us to rewrite the equation as:
\begin{equation}
    \mqty( 1 \\ y ) = \mqty( 1 & 0 \cdots 0 \\ b & W ) \mqty( 1 \\ x )
\end{equation}

The second key difference is that, instead of feeding one data point at a time to the network and heavily relying on inefficient for loops, we can feed a batch of $m$ data points at once, and leverage efficient matrix operations. This means that the input $x$ is actually a matrix $X$ where each column represents a different data point, and the output $y$ is also a matrix $Y$ where each column corresponds to the output for each input data point. The equation then becomes:
\begin{equation}
    \mqty( 1 \cdots 1 \\ Y ) = \mqty( 1 & 0 \cdots 0 \\ b & W ) \mqty( 1 \cdots 1 \\ X )
\end{equation}

Switching to the backpropagation notation introduced in \cref{sec:backprop}, we can summarize the forward pass of a Linear layer as:
\begin{equation}
    Z^{[l]} = W^{[l]} A^{[l - 1]}
\end{equation}
where:
\begin{itemize}
    \item $A^{[l - 1]}$ is the \shape{(\nin + 1)}{m} input of the Linear layer $l$, with $A^{[0]}$ being the input data,
    \item $W^{[l]}$ is the \shape{(\nout + 1)}{(\nin + 1)} weights matrix of the Linear layer $l$, and
    \item $Z^{[l]}$ is the \shape{(\nout + 1)}{m} output of the Linear layer $l$.
\end{itemize}
All these Tensors already include the necessary additions to correctly handle the bias.

The Linear layer forward method also stores its input $A^{[l - 1]}$ for use in the backward pass.

\subsubsection{Backward pass}
The Linear layer's responsibility is to compute the gradients of the loss \wrt its weights during the backward pass and pass backward the gradient of the loss \wrt its input. Mathematically, this can be expressed as:
\begin{gather}
    \pdv{\J}{W^{[l]}} = \pdv{\J}{Z^{[l]}} \pdv{Z^{[l]}}{W^{[l]}} = \Delta^{[l]} A^{[l - 1]T}\\
    \pdv{\J}{A^{[l - 1]}} = \pdv{Z^{[l]}}{A^{[l - 1]}} \pdv{\J}{Z^{[l]}} = W^{[l]T} \Delta^{[l]}
\end{gather}
where $\J$ is the loss and $\Delta^{[l]} = \pdv{\J}{Z^{[l]}}$ is the gradient of the loss \wrt the output of the Linear layer $l$.

\paragraph{Implementation Details} The backward method of the Linear layer takes as input $\Delta^{[l]}$ and computes the gradient \wrt the weights $W^{[l]}$. This gradient is then stored in the layer for later use during the optimization step, when it will be used to update the weights.

\subsection{Activation Layer}
\subsubsection{Forward pass}
The role of the Activation layer is to apply a non-linear activation function $g$ element-wise to the output of the previous Linear layer. This non-linearity is crucial for the \acl{NN} to learn complex patterns in the data.

\paragraph{Implementation Details} The forward method of the Activation layer takes as input the output of the Linear layer $Z^{[l]}$ and applies the activation function element-wise to produce the activated output $A^{[l]}$:
\begin{equation}
    A^{[l]} = g(Z^{[l]})
\end{equation}

This step is rendered more complex by the bias feature, which must be preserved and propagated to the next layer without any modification. Therefore, the activation function is applied only to the rows of $Z^{[l]}$ corresponding to actual features, leaving the first row (the bias feature) unchanged.

The Activation layer forward method also stores its input $Z^{[l]}$ for use in the backward pass.

Only two types of activation functions are implemented in \mfnet, ReLU and Sigmoid, defined as follows:
\begin{itemize}
    \item ReLU: $g(x) = \max(0, x)$
    \item Sigmoid: $g(x) = \flatfrac{1}{(1 + e^{-x})}$
\end{itemize}

\subsubsection{Backward pass}
Since the Activation layer does not have any learnable parameters, its backward method is solely responsible for computing the gradient of the loss \wrt its input $Z^{[l]}$:
\begin{equation}
    \pdv{\J}{Z^{[l]}} = \pdv{\J}{A^{[l]}} \odot g'(Z^{[l]})
\end{equation}
where $g'$ is the derivative of the activation function and $\odot$ denotes element-wise multiplication (Hadamard product).

\paragraph{Implementation Details} The backward method of the Activation layer takes as input $\pdv{\J}{A^{[l]}}$ and computes $\pdv{\J}{Z^{[l]}}$. Similar to the forward pass, the bias feature must be preserved during this computation. Therefore, the derivative of the activation function is applied only to the rows corresponding to actual features, setting the first row (the bias feature) to zero. This ensures that the first row of the weight matrix of the previous Linear layer does not get updated during the optimization step, and therefore that the bias feature gets correctly propagated forward through the network.
