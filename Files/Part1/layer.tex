\section{Layer}

In \mfnet, a layer is a fundamental building block of the neural network. Each layer consists of a set of neurons, and it performs a specific transformation on the input data. The two main types of layers implemented in \mfnet are Linear layers and Activation layers.

% \begin{itemize}
%     \item \textbf{Linear Layer:} This layer applies a linear transformation to the input data, represented by the equation $y = Ax + b$, where $A$ is the weight matrix, $x$ is the input vector, $b$ is the bias vector, and $y$ is the output vector.
%     \item \textbf{Activation Layer:} This layer applies a non-linear activation function to the output of the linear layer. Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.
% \end{itemize}

Layers are stacked together to form a complete neural network, with the output of one layer serving as the input to the next layer.  Each layer is responsible for maintaining its own parameters and computing gradients during the backpropagation process.

% Each of the layers in \mfnet expects the input to be in the form of a matrix with shape $(n_{\text{in}} + 1, m)$, where $n_{\text{in}}$ is the number of input features and $m$ is the number of input samples. The first row of this matrix is reserved for a bias term, which is always set to 1.

\subsection{Linear Layer}
\subsubsection{Forward pass}
The Linear layer in \mfnet applies a linear transformation to the input data, performing a change in dimensionality from $\nin$ to $\nout$, where $\nin$ is the number of input features and $\nout$ is the number of output features. Mathematically, this can be represented as:
\begin{equation}
    y = b + Wx
\end{equation}

where:
\begin{itemize}
    \item $x$ is the \shape{\nin}{1} input vector,
    \item $W$ is the \shape{\nout}{\nin} weight matrix,
    \item $b$ is the \shape{\nout}{1} bias vector, and
    \item $y$ is the \shape{\nout}{1} output vector.
\end{itemize}

\paragraph{Implementation Details} The actual implementation is a bit different: the first key difference is that the bias term is absorbed inside the weights matrix, and the input vector is augmented with an additional constant value of 1. The layer expects this ``bias feature'' to be already present in the input data, and propagates it to the next layer by adding a row of $\mqty(1 & 0 \cdots 0)$ to the weights matrix. This allows us to rewrite the equation as:
\begin{equation}
    \mqty( 1 \\ y ) = \mqty( 1 & 0 \cdots 0 \\ b & W ) \mqty( 1 \\ x )
\end{equation}

The second key difference is that, instead of feeding one data point at a time to the network and heavily relying on inefficient for loops, we can feed a batch of $m$ data points at once, and leverage efficient matrix operations. This means that the input $x$ is actually a matrix $X$ where each column represents a different data point, and the output $y$ is also a matrix $Y$ where each column corresponds to the output for each input data point. The equation then becomes:
\begin{equation}
    \mqty( 1 \cdots 1 \\ Y ) = \mqty( 1 & 0 \cdots 0 \\ b & W ) \mqty( 1 \cdots 1 \\ X )
\end{equation}

Switching to the backpropagation notation introduced in \cref{sec:backprop}, we can summarize the forward pass of a Linear layer as:
\begin{equation}
    Z^{[l]} = W^{[l]} A^{[l - 1]}
\end{equation}
where:
\begin{itemize}
    \item $A^{[l - 1]}$ is the \shape{(\nin + 1)}{m} input of the Linear layer $l$, with $A^{[0]}$ being the input data,
    \item $W^{[l]}$ is the \shape{(\nout + 1)}{(\nin + 1)} weights matrix of the Linear layer $l$, and
    \item $Z^{[l]}$ is the \shape{(\nout + 1)}{m} output of the Linear layer $l$.
\end{itemize}
All these Tensors already include the necessary additions to correctly handle the bias.

The Linear layer forward method also stores its input $A^{[l - 1]}$ for use in the backward pass.

\subsubsection{Backward pass}
The Linear layer's responsibility is to compute the gradients of the loss \wrt its weights during the backward pass and pass backward the gradient of the loss \wrt its input. Mathematically, this can be expressed as:
\begin{gather}
    \pdv{\J}{W^{[l]}} = \pdv{\J}{Z^{[l]}} \pdv{Z^{[l]}}{W^{[l]}} = \Delta^{[l]} A^{[l - 1]T}\\
    \pdv{\J}{A^{[l - 1]}} = \pdv{Z^{[l]}}{A^{[l - 1]}} \pdv{\J}{Z^{[l]}} = W^{[l]T} \Delta^{[l]}
\end{gather}
where $\J$ is the loss and $\Delta^{[l]} = \pdv{\J}{Z^{[l]}}$ is the gradient of the loss \wrt the output of the Linear layer $l$.

\paragraph{Implementation Details} The backward method of the Linear layer takes as input $\Delta^{[l]}$ and computes the gradient \wrt the weights $W^{[l]}$. This gradient is then stored in the layer for later use during the optimization step, when it will be used to update the weights.

\subsection{Activation Layer}
\subsubsection{Forward pass}
The role of the Activation layer is to apply a non-linear activation function $g$ element-wise to the output of the previous Linear layer. This non-linearity is crucial for the neural network to learn complex patterns in the data.

\paragraph{Implementation Details} The forward method of the Activation layer takes as input the output of the Linear layer $Z^{[l]}$ and applies the activation function element-wise to produce the activated output $A^{[l]}$:
\begin{equation}
    A^{[l]} = g(Z^{[l]})
\end{equation}

This step is rendered more complex by the bias feature, which must be preserved and propagated to the next layer without any modification. Therefore, the activation function is applied only to the rows of $Z^{[l]}$ corresponding to actual features, leaving the first row (the bias feature) unchanged.

The Activation layer forward method also stores its input $Z^{[l]}$ for use in the backward pass.

Only two types of activation functions are implemented in \mfnet, ReLU and Sigmoid, defined as follows:
\begin{itemize}
    \item ReLU: $g(x) = \max(0, x)$
    \item Sigmoid: $g(x) = \flatfrac{1}{(1 + e^{-x})}$
\end{itemize}

\subsubsection{Backward pass}
Since the Activation layer does not have any learnable parameters, its backward method is solely responsible for computing the gradient of the loss \wrt its input $Z^{[l]}$:
\begin{equation}
    \pdv{\J}{Z^{[l]}} = \pdv{\J}{A^{[l]}} \odot g'(Z^{[l]})
\end{equation}
where $g'$ is the derivative of the activation function and $\odot$ denotes element-wise multiplication (Hadamard product).

\paragraph{Implementation Details} The backward method of the Activation layer takes as input $\pdv{\J}{A^{[l]}}$ and computes $\pdv{\J}{Z^{[l]}}$. Similar to the forward pass, the bias feature must be preserved during this computation. Therefore, the derivative of the activation function is applied only to the rows corresponding to actual features, setting the first row (the bias feature) to zero. This ensures that the first row of the weight matrix of the previous Linear layer does not get updated during the optimization step, and therefore that the bias feature gets correctly propagated forward through the network.

\subsection{Example}
To better understand the workings of the layers, it's helpful to consider a practical example: we'll walk through a forward/backward cycle of a small network with two hidden layers and all the activation functions set to the identity.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=2.2cm,y=1.4cm]
        \readlist\Nnod{3,4,3,2} % array of number of nodes per layer

        \foreachitem \N \in \Nnod{ % loop over layers
            \def\curr{\Ncnt} % alias of index of current layer
            \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
            \foreach \i [evaluate={\y=\N/2-\i; \x=\curr; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes

                % NODES
                \ifnum\i=1
                    \node[node 0] (N\curr-\i) at (\x,\y) {};
                \else
                    \node[node \n] (N\curr-\i) at (\x,\y) {};
                \fi

                % CONNECTIONS
                \ifnum\curr>1 % connect to previous layer
                    \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                        \draw[connect,white,line width=1.2] (N\prev-\j) -- (N\curr-\i);
                        \draw[connect] (N\prev-\j) -- (N\curr-\i);
                    }
                \fi

            }
        }
    \end{tikzpicture}
    \caption{A small neural network with two hidden layers. The first layer is a Linear layer with 2 input features (plus bias) and 3 output features (plus bias). The second layer is a Linear layer with 3 input features (plus bias) and 2 output features (plus bias). The third layer is a Linear layer with 2 input features (plus bias) and 1 output feature (plus bias). All activation functions are set to the identity.}
    \label{fig:small-nn}
\end{figure}

Let's pick an input $X$ and target $Y$:
\begin{equation*}
    X = \begin{bmatrix}
        1 & 1 & 2 & 2 \\
        2 & 2 & 2 & 3
    \end{bmatrix}, \quad
    Y = \begin{bmatrix}
        1 & 1 & 2 & 2
    \end{bmatrix}
\end{equation*}

This is a small dataset of four samples with two input features and one output feature.

The first step is to prepend the bias feature to the input and to the output:
\begin{equation*}
    A^{[0]} = \tilde{X} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        1 & 1 & 2 & 2 \\
        2 & 2 & 2 & 3
    ], \quad \tilde{Y} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        1 & 1 & 2 & 2
    ]
\end{equation*}

\subsubsection{Forward pass}
\paragraph{Layer 1 (Linear)} The first layer is a Linear layer with 3 input features (plus bias) and 4 output features (plus bias). The weights matrix $W^{[1]}$ is initialized randomly:
\begin{equation*}
    W^{[1]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 0
    ]
\end{equation*}
\begin{equation*}
    Z^{[1]} = W^{[1]} A^{[0]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 3 & 2 & 3 \\
        3 & 4 & 4 & 5 \\
        2 & 2 & 3 & 3
    ]
\end{equation*}

\paragraph{Layer 1 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    A^{[1]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[1]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 3 & 2 & 3 \\
        3 & 4 & 4 & 5 \\
        2 & 2 & 3 & 3
    ]
\end{equation*}

\paragraph{Layer 2 (Linear)} The second layer is a Linear layer with 4 input features (plus bias) and 3 output features (plus bias). The weights matrix $W^{[2]}$ is initialized randomly:
\begin{equation*}
    W^{[2]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 1 & 0 & 1\\
        -1 & 1 & 2 & -2
    ]
\end{equation*}
\begin{equation*}
    Z^{[2]} = W^{[2]} A^{[1]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        5 & 6 & 6 & 7 \\
        3 & 6 & 3 & 6
    ]
\end{equation*}

\paragraph{Layer 2 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    A^{[2]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[2]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        5 & 6 & 6 & 7 \\
        3 & 6 & 3 & 6
    ]
\end{equation*}

\paragraph{Layer 3 (Linear)} The third layer is a Linear layer with 3 input features (plus bias) and 2 output features (plus bias). The weights matrix $W^{[3]}$ is initialized randomly:
\begin{equation*}
    W^{[3]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 1 & -1
    ]
\end{equation*}
\begin{equation*}
    Z^{[3]} = W^{[3]} A^{[2]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 0 & 3 & 1
    ]
\end{equation*}

\paragraph{Layer 3 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    \hat{Y} = A^{[3]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[3]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 0 & 3 & 1
    ]
\end{equation*}

Now we compute the loss:
\begin{align*}
    \J(\hat{Y}, \tilde{Y}) &= \frac{1}{m} \sum_{i=1}^{m} \norm{\hat{Y}_i - \tilde{Y}_i}^2\\
    &\;\begin{aligned}
        = \frac{1}{4} \left[\right.&(1 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + \\
        &(2 - 1)^2 + (0 - 1)^2 + (3 - 2)^2 + (1 - 2)^2\left.\right] = 1
    \end{aligned}
\end{align*}

\subsubsection{Backward pass}
To start the backward pass, we need to compute the gradient of the loss \wrt the output of the network:
\begin{equation*}
    \frac{\partial \J}{\partial \hat{Y}} = \frac{2}{m} \left(\hat{Y} - \tilde{Y}\right) = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1
    ] = \pdv{\J}{A^{[3]}}
\end{equation*}
This is now the input of the backward method of the last layer.

\paragraph{Layer 3 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[3]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[3]} = \pdv{\J}{Z^{[3]}} = \pdv{\J}{A^{[3]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[3]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1
    ]
\end{equation*}

\paragraph{Layer 3 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[3]}$ and its input $A^{[2]}$:
\begin{gather*}
    \pdv{\J}{W^{[3]}} = \Delta^{[3]} A^{[2]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -6
    ]\\
    \pdv{\J}{A^{[2]}} = W^{[3]T} \Delta^{[3]} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1 \\
        -1 & 1 & -1 & 1
    ]
\end{gather*}

\paragraph{Layer 2 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[2]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[2]} = \pdv{\J}{Z^{[2]}} = \pdv{\J}{A^{[2]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[2]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1 \\
        -1 & 1 & -1 & 1
    ]
\end{equation*}

\paragraph{Layer 2 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[2]}$ and its input $A^{[1]}$:
\begin{gather*}
    \pdv{\J}{W^{[2]}} = \Delta^{[2]} A^{[1]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -2 & 0 \\
        0 & 2 & 2 & 0 \\
    ]\\
    \pdv{\J}{A^{[1]}} = W^{[2]T} \Delta^{[2]} = \frac{1}{4} \mqty[
        \color{red} 2 & \color{red} -2 & \color{red} 2 & \color{red} -2 \\
        0 & 0 & 0 & 0 \\
        -2 & 2 & -2 & 2 \\
        3 & -3 & 3 & -3 \\
    ]
\end{gather*}

\paragraph{Layer 1 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[1]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[1]} = \pdv{\J}{Z^{[1]}} = \pdv{\J}{A^{[1]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[1]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 & 0 \\
        -2 & 2 & -2 & 2 \\
        3 & -3 & 3 & -3 \\
    ]
\end{equation*}

\paragraph{Layer 1 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[1]}$ and its input $A^{[0]}$:
\begin{gather*}
    \pdv{\J}{W^{[1]}} = \Delta^{[1]} A^{[0]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 \\
        0 & 0 & 4 \\
        0 & 0 & -6
    ]\\
    \pdv{\J}{A^{[0]}} = W^{[1]T} \Delta^{[1]} = \frac{1}{4} \mqty[
        \color{red} 3 & \color{red} -3 & \color{red} 3 & \color{red} -3 \\
        1 & -1 & 1 & -1 \\
        -2 & 2 & -2 & 2
    ] \quad \text{(unused)}
\end{gather*}

\subsubsection{Weight update}

The weights are updated using gradient descent:
\begin{equation*}
    W^{[l]} \gets W^{[l]} - \eta \pdv{\J}{W^{[l]}} \\
\end{equation*}

Setting the learning rate $\eta = 4$ for simplicity\footnote{This value is way too high to have any chance of yielding an improvement in the predictions in any practical example.}, we have: 
\begin{gather*}
    W^{[3]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 1 & -1
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -6
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 3 & 5
    ]\\
    W^{[2]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 1 & 0 & 1\\
        -1 & 1 & 2 & -2
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -2 & 0 \\
        0 & 2 & 2 & 0 \\
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 3 & 2 & 1\\
        -1 & -1 & 0 & -2
    ]\\
    W^{[1]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 0
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 \\
        0 & 0 & 4 \\
        0 & 0 & -6
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & -3 \\
        1 & 0 & 6
    ]
\end{gather*}

Now a new cycle of forward pass/backward pass/weight update can begin.
