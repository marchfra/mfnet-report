\section{Backpropagation} \label{sec:backprop}
Before diving into the details of the implementation of \mfnet, it's necessary to understand the algorithm at the core of the learning process: backpropagation.

The algorithm is more easily understandable using the index notation, but in order to implement it in code, we need to express it in matrix form, so both formulations will be shown.

The goal of the backpropagation algorithm is to compute the derivative of the loss \wrt each weight in the network using the chain rule.

\subsection{Index notation} \label{par:index_notation}
We start by defining the following quantities:
\begin{itemize}
    \item $L$: the total number of layers in the network;
    \item $n^{[l]}$: the number of neurons in layer $l$ (with $l = 1$ being the input layer, and $l = L$ being the output layer);
    \item $z_j^{[l]}$: the pre-activation of neuron $j$ in layer $l$ (with $z_1^{[l]} = 1$). This is the output of the linear transformation in layer $l$ and the input of the activation function;
    \item $a_j^{[l]}$: the activation of neuron $j$ in layer $l$ (with $a_1^{[l]} = 1$, $a_j^{[0]} = x_j$, and $a_j^{[L]} = \hat{y}_j$). This is the output of layer $l$ and the input of layer $l+1$;
    \item $W_{jk}^{[l]}$: the weight connecting neuron $k$ in layer $l-1$ to neuron $j$ in layer $l$ (with $W_{j1}^{[l]} = b_j^{[l]}$ and $W_{1k}^{[l]} = \delta_{1k}$);
    \item $g$: the activation function used in the hidden layers;
    \item $f$: the activation function used in the output layer.
\end{itemize}

The flow of information through layer $l$ is given by:
\begin{gather*}
    z_j^{[l]} = \sum_{k=1}^{n^{[l-1]}} W_{jk}^{[l]} a_k^{[l-1]} \\
    a_j^{[l]} = g\left(z_j^{[l]}\right)
\end{gather*}

The derivative of the loss $\J$ \wrt the weight $W_{jk}^{[l]}$ is computed as:
\begin{align*}
    \pdv{\J}{W_{jk}^{[l]}} &= \pdv{\J}{z_j^{[l]}} \pdv{z_j^{[l]}}{W_{jk}^{[l]}}\\
    &= \Delta_j^{[l]} \pdv{z_j^{[l]}}{W_{jk}^{[l]}} = \Delta_j^{[l]} a_k^{[l-1]}
\end{align*}

Now, $\Delta_j^{[l]}$ can be expressed as a function of $\Delta_j^{[l + 1]}$:
\begin{align*}
    \Delta_j^{[l]} = \pdv{\J}{z_j^{[l]}} &= \pdv{\J}{a_j^{[l]}} \pdv{a_j^{[l]}}{z_j^{[l]}}\\
    &= \left( \sum_{i=1}^{n^{[l + 1]}} \pdv{\J}{z_i^{[l+1]}} \pdv{z_i^{[l+1]}}{a_j^{[l]}} \right) \pdv{a_j^{[l]}}{z_j^{[l]}}\\
    &= \left( \sum_{i=1}^{n^{[l + 1]}} \Delta_i^{[l+1]} W_{ij}^{[l+1]} \right) g'\left(z_j^{[l]}\right)
\end{align*}

The procedure can be iterated until the last layer $L$ is reached:
\begin{align*}
    \Delta_j^{[L]} = \pdv{\J}{z_j^{[L]}} = \pdv{\J}{a_j^{[L]}} \pdv{a_j^{[L]}}{z_j^{[L]}} = \pdv{\J}{\hat{y}_j} f'\left(z_j^{[L]}\right)
\end{align*}

This last term can be computed after the forward pass is completed. By iteration, every $\Delta_j^{[l]}$ can be computed, and thus every $\pdv{\J}{W_{jk}^{[l]}}$.

\subsection{Matrix notation} Matrix notation can be easily derived from the index notation, being careful with the order of the products and with placing the transposes.

The definitions given in the \nameref{par:index_notation} paragraph are updated as follows:
\begin{itemize}
    \item $m$: the number of samples in the training batch;
    \item $Z^{[l]}$: the \shape{n^{[l]}}{m} pre-activation of layer $l$;
    \item $A^{[l]}$: the \shape{n^{[l]}}{m} activation of layer $l$;
    \item $W^{[l]}$: the \shape{n^{[l]}}{n^{[l-1]}} weight matrix of layer $l$.
\end{itemize}

The flow of information through layer $l$ is given by:
\begin{gather}
    Z^{[l]} = W^{[l]} A^{[l-1]} \label{eq:lin_forward}\\
    A^{[l]} = g\left(Z^{[l]}\right) \label{eq:act_forward}
\end{gather}

The derivative of the loss $\J$ \wrt the weights $W^{[l]}$ is computed as:
\begin{equation} \label{eq:weight_grad}
    \pdv{\J}{W^{[l]}} = \pdv{\J}{Z^{[l]}} \pdv{Z^{[l]}}{W^{[l]}} = \Delta^{[l]} A^{[l-1]T}
\end{equation}

Now, $\Delta^{[l]}$ can be expressed as a function of $\Delta^{[l + 1]}$:
\begin{equation} \label{eq:delta_recursion}
\begin{split}
    \Delta^{[l]} = \pdv{\J}{Z^{[l]}} &= \pdv{\J}{A^{[l]}} \odot \pdv{A^{[l]}}{Z^{[l]}}\\
    &= \left(\pdv{Z^{[l+1]}}{A^{[l]}} \pdv{\J}{Z^{[l+1]}} \right) \odot \pdv{A^{[l]}}{Z^{[l]}}\\
    &= \left(W^{[l+1]T} \Delta^{[l+1]} \right) \odot g'\left(Z^{[l]}\right)
\end{split}
\end{equation}
where $\odot$ denotes the element-wise (Hadamard) product.

The procedure can be iterated until the last layer $L$ is reached:
\begin{equation*}
    \Delta^{[L]} = \pdv{\J}{Z^{[L]}} = \pdv{\J}{A^{[L]}} \odot \pdv{A^{[L]}}{Z^{[L]}} = \pdv{\J}{\hat{Y}} \odot f'\left(Z^{[L]}\right)
\end{equation*}

This last term can be computed after the forward pass is completed. By iteration, every $\Delta^{[l]}$ can be computed using \cref{eq:delta_recursion}, and thus every $\pdv{\J}{W^{[l]}}$.

\paragraph{Note on bias augmentation} In the implementation, tensors always include bias: $A^{[l]}$ includes a leading row of ones and $W^{[l]}$ includes the corresponding bias row and column (see \cref{sec:layer}). The formulas above map directly by treating those rows/columns as fixed: the activation derivative on the bias row is zeroed in backward passes, and the first row of the gradient for logits/layer inputs is handled accordingly so that biases propagate but are not transformed by nonlinearities. This will be explained in greater detail in \cref{sec:layer,sec:loss}, and shown in the practical example in \cref{sec:example}.
