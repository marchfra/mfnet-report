\section{Step-by-step example of an epoch} \label{sec:example}
To better understand how the bias is handled throughout \mfnet, it's helpful to consider a practical example: we'll walk through a forward/backward cycle of a small network with two hidden layers and all the activation functions set to the identity.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[x=2.2cm,y=1.4cm]
        \readlist\Nnod{3,4,3,2} % array of number of nodes per layer

        \foreachitem \N \in \Nnod{ % loop over layers
            \def\curr{\Ncnt} % alias of index of current layer
            \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
            \foreach \i [evaluate={\y=\N/2-\i; \x=\curr; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes

                % NODES
                \ifnum\i=1
                    \node[node 0] (N\curr-\i) at (\x,\y) {};
                \else
                    \node[node \n] (N\curr-\i) at (\x,\y) {};
                \fi

                % CONNECTIONS
                \ifnum\curr>1 % connect to previous layer
                    \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                        \draw[connect,white,line width=1.2] (N\prev-\j) -- (N\curr-\i);
                        \draw[connect] (N\prev-\j) -- (N\curr-\i);
                    }
                \fi

            }
        }
    \end{tikzpicture}
    \caption{A small \acl{NN} with two hidden layers. The gray neurons represent the bias features.}
    \label{fig:small-nn}
\end{figure}

Let's pick an input $X$ and target $Y$:
\begin{equation*}
    X = \mqty[
        1 & 2 \\
        1 & 2 \\
        2 & 2 \\
        2 & 3
    ], \quad
    Y = \mqty[1 \\ 1 \\ 2 \\ 2]
\end{equation*}

This is a small dataset of four samples with two input features and one target feature.

First, the \mono{DataLoader} transposes the data and prepends the bias feature:
\begin{equation*}
    A^{[0]} = \tilde{X} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        1 & 1 & 2 & 2 \\
        2 & 2 & 2 & 3
    ], \quad \tilde{Y} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        1 & 1 & 2 & 2
    ]
\end{equation*}

\newpage
\subsection{Forward pass}
Now the \mono{forward} method of the \mono{NeuralNetwork} is called with input data $A^{[0]}$.

\paragraph{Layer 1 (Linear)} The first layer is a Linear layer with 2 input features (plus bias) and 3 output features (plus bias). Suppose the weights matrix $W^{[1]}$ is:
\begin{equation*}
    W^{[1]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 0
    ]
\end{equation*}

Then, the output of the layer is:
\begin{equation*}
    Z^{[1]} = W^{[1]} A^{[0]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 3 & 2 & 3 \\
        3 & 4 & 4 & 5 \\
        2 & 2 & 3 & 3
    ]
\end{equation*}

\paragraph{Layer 1 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    A^{[1]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[1]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 3 & 2 & 3 \\
        3 & 4 & 4 & 5 \\
        2 & 2 & 3 & 3
    ]
\end{equation*}

\paragraph{Layer 2 (Linear)} The second layer is a Linear layer with 3 input features (plus bias) and 2 output features (plus bias). Suppose the weights matrix $W^{[2]}$ is:
\begin{equation*}
    W^{[2]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 1 & 0 & 1\\
        -1 & 1 & 2 & -2
    ]
\end{equation*}

Then, the output of the layer is:
\begin{equation*}
    Z^{[2]} = W^{[2]} A^{[1]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        5 & 6 & 6 & 7 \\
        3 & 6 & 3 & 6
    ]
\end{equation*}

\paragraph{Layer 2 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    A^{[2]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[2]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        5 & 6 & 6 & 7 \\
        3 & 6 & 3 & 6
    ]
\end{equation*}

\paragraph{Layer 3 (Linear)} The third layer is a Linear layer with 2 input features (plus bias) and 1 output feature (plus bias). Suppose the weights matrix $W^{[3]}$ is:
\begin{equation*}
    W^{[3]} = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 1 & -1
    ]
\end{equation*}

Then, the output of the layer is:
\begin{equation*}
    Z^{[3]} = W^{[3]} A^{[2]} = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 0 & 3 & 1
    ]
\end{equation*}

\paragraph{Layer 3 (Activation)} The activation function is applied element-wise, skipping the first row:
\begin{equation*}
    \hat{Y} = A^{[3]} = \mqty[
        \mqty{\color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1} \\
        g(Z^{[3]}[1\hspace{-0.6ex}:])
    ] = \mqty[
        \color{red} 1 & \color{red} 1 & \color{red} 1 & \color{red} 1 \\
        2 & 0 & 3 & 1
    ]
\end{equation*}

Now we compute the loss, for instance \ac{MSE}:
\begin{align*}
    \J(\hat{Y}, \tilde{Y}) &= \frac{1}{m} \sum_{i=1}^{m} \norm{\hat{Y}_i - \tilde{Y}_i}^2\\
    &\;\begin{aligned}
        = \frac{1}{4} \left[\right.&(1 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + \\
        &+ (2 - 1)^2 + (0 - 1)^2 + (3 - 2)^2 + (1 - 2)^2\left.\right] = 1
    \end{aligned}
\end{align*}

\subsection{Backward pass}
To start the backward pass, we need to compute the gradient of the loss \wrt the output of the network:
\begin{equation*}
    \frac{\partial \J}{\partial \hat{Y}} = \frac{2}{m} \left(\hat{Y} - \tilde{Y}\right) = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1
    ] = \pdv{\J}{A^{[3]}}
\end{equation*}
This is now the input of the \mono{backward} method of the \mono{NeuralNetwork}.

\paragraph{Layer 3 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[3]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[3]} = \pdv{\J}{Z^{[3]}} = \pdv{\J}{A^{[3]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[3]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1
    ]
\end{equation*}

\paragraph{Layer 3 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[3]}$ and its input $A^{[2]}$:
\begin{gather*}
    \pdv{\J}{W^{[3]}} = \Delta^{[3]} A^{[2]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -6
    ]\\
    \pdv{\J}{A^{[2]}} = W^{[3]T} \Delta^{[3]} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1 \\
        -1 & 1 & -1 & 1
    ]
\end{gather*}

\paragraph{Layer 2 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[2]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[2]} = \pdv{\J}{Z^{[2]}} = \pdv{\J}{A^{[2]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[2]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        1 & -1 & 1 & -1 \\
        -1 & 1 & -1 & 1
    ]
\end{equation*}

\paragraph{Layer 2 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[2]}$ and its input $A^{[1]}$:
\begin{gather*}
    \pdv{\J}{W^{[2]}} = \Delta^{[2]} A^{[1]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -2 & 0 \\
        0 & 2 & 2 & 0 \\
    ]\\
    \pdv{\J}{A^{[1]}} = W^{[2]T} \Delta^{[2]} = \frac{1}{4} \mqty[
        \color{red} 2 & \color{red} -2 & \color{red} 2 & \color{red} -2 \\
        0 & 0 & 0 & 0 \\
        -2 & 2 & -2 & 2 \\
        3 & -3 & 3 & -3 \\
    ]
\end{gather*}

\paragraph{Layer 1 (Activation)} The backward method of the Activation layer computes the gradient of the loss \wrt its input $Z^{[1]}$. Since the activation function is the identity, its derivative is 1, and we have:
\begin{equation*}
    \Delta^{[1]} = \pdv{\J}{Z^{[1]}} = \pdv{\J}{A^{[1]}} \odot \mqty[
        \mqty{\color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0} \\
        g'(Z^{[1]}[1\hspace{-0.6ex}:])
    ] = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 & 0 \\
        -2 & 2 & -2 & 2 \\
        3 & -3 & 3 & -3 \\
    ]
\end{equation*}

\paragraph{Layer 1 (Linear)} The backward method of the Linear layer computes the gradient of the loss \wrt its weights $W^{[1]}$ and its input $A^{[0]}$:
\begin{gather*}
    \pdv{\J}{W^{[1]}} = \Delta^{[1]} A^{[0]T} = \frac{1}{4} \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 \\
        0 & 0 & 4 \\
        0 & 0 & -6
    ]\\
    \pdv{\J}{A^{[0]}} = W^{[1]T} \Delta^{[1]} = \frac{1}{4} \mqty[
        \color{red} 3 & \color{red} -3 & \color{red} 3 & \color{red} -3 \\
        1 & -1 & 1 & -1 \\
        -2 & 2 & -2 & 2
    ] \quad \text{(unused)}
\end{gather*}

\subsection{Weight update}

The weights are updated using the \ac{SGD} optimizer:
\begin{equation*}
    W^{[l]} \gets W^{[l]} - \eta \pdv{\J}{W^{[l]}} \\
\end{equation*}

Setting the learning rate $\eta = 4$ for simplicity\footnote{This value is way too high to have any chance of yielding an improvement in the predictions in any practical example.}, we have: 
\begin{gather*}
    W^{[3]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 1 & -1
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -6
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 3 & 5
    ]\\
    W^{[2]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 1 & 0 & 1\\
        -1 & 1 & 2 & -2
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & -2 & -2 & 0 \\
        0 & 2 & 2 & 0 \\
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 & \color{red} 0\\
        1 & 3 & 2 & 1\\
        -1 & -1 & 0 & -2
    ]\\
    W^{[1]} \gets \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 0 & 0
    ] - \mqty[
        \color{red} 0 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 0 \\
        0 & 0 & 4 \\
        0 & 0 & -6
    ] = \mqty[
        \color{red} 1 & \color{red} 0 & \color{red} 0 \\
        0 & 0 & 1 \\
        0 & 1 & -3 \\
        1 & 0 & 6
    ]
\end{gather*}

Now a new cycle of forward pass/backward pass/weight update can begin.
