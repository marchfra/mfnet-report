% \addcontentsline{toc}{section}{Signal Flow in \aclp*{NN}}
\section*{Signal Flow in \aclp*{NN}}

A \ac{FFCNN} is a \ac{NN} architecture where each neuron in one layer is connected to every neuron in the subsequent layer. A \ac{FFCNN} learns by iteratively performing two main steps: the forward pass and the backward pass.

\begin{figure}[h]
    \centering
    % NEURAL NETWORK with coefficients, arrows
    \begin{tikzpicture}[x=2.2cm,y=1.4cm]
    \readlist\Nnod{4,5,5,5,3} % array of number of nodes per layer

    \foreachitem \N \in \Nnod{ % loop over layers
        \edef\curr{\Ncnt} % alias of index of current layer
        \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
        \foreach \i [evaluate={\y=\N/2-\i; \x=\curr; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes

        % NODES
        \node[node \n] (N\curr-\i) at (\x,\y) {};

        % CONNECTIONS
        \ifnum\curr>1 % connect to previous layer
            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                \draw[connect arrow] (N\prev-\j) -- (N\curr-\i); % connect arrows directly
            }
        \fi

        }

    }

    % LABELS
    \node[above=0.3,align=center,mygreen!60!black] at (N1-1.90) {input\\[-0.2em]layer};
    \node[above=0.1,align=center,myblue!60!black] at (N3-1.90) {hidden layers};
    \node[above=0.8,align=center,myred!60!black] at (N\Nnodlen-1.90) {output\\[-0.2em]layer};

    \end{tikzpicture}
    \caption{\Iac{FFCNN}.}
    \label{fig:fcnn}
\end{figure}

\paragraph{Forward pass} The input data is propagated through the network, layer by layer, to produce an output prediction. This prediction is then compared to the true target values using a loss function, which quantifies the prediction error.

\paragraph{Backward pass} The network uses the computed loss to adjust its internal parameters. This is done by propagating the error backward through the network and updating the weights to minimize the loss. The process of forward and backward passes is repeated for multiple iterations, gradually improving the model's performance. This step is the heart of the learning process, as it allows the network to learn from its mistakes and improve its predictions over time.
